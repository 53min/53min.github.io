---
title: "[인공지능] 딥러닝"
author: <author_id>
date: 2024-03-19 18:00:00 +09:00
categories: [AI, Deep Learning]
math: true
tag: [자연어처리, 딥러닝, NLP, Deep Learning, 퍼셉트론, 인공 신경망, 활성화 함수, 역전파, 기울기 소실, 피드 포워드 신경망]
toc: true
---

> 이 포스트는 [**딥 러닝을 이용한 자연어 처리 입문**](https://wikidocs.net/book/2155)을 읽고 공부 겸 정리한 글입니다.

## **딥러닝**

**딥러닝(Deep Learning)**은 머신 러닝(Machine Leraning)의 여러 방법론 중에 하나이다. 최근까지 높은 성능으로 화두에 있으며, 인간의 뇌를 모방한 인공 신경망을 연속적으로 층을 쌓아 학습하는 방식이다.

## **퍼셉트론(Perceptron)**

**퍼셉트론**이란 가장 먼저 제안된 초기 형태의 인공 신경망이다. 다수의 입력을 받아 하나의 결과를 출력하는 알고리즘으로 뇌를 구성하는 신경 세포인 뉴런의 동작과 유사하다. 뉴런은 가지돌기에서 신호를 받아 신호가 일정 크기를 넘어가면 축삭돌기를 통해 신호를 전달한다. 퍼셉트론 역시 다수의 입력을 받으며 입력의 수 만큼 가중치가 존재하며, 입력 값이 중요할수록 큰 가중치를 가진다. 편향 또한 입력으로 사용되며, 입력값이 1로 고정되고 편향 b가 곱해지는 변수로 표현된다. 입력값과 가중치의 곱의 총 합이 임계치(threshold)를 넘으면 1을 출력하고, 그렇지 않다면 0을 출력한다. 이러한 함수를 **계단 함수(Step Function)**이라고 한다.

![Desktop View](/assets/img/퍼셉트론2.png)
_퍼셉트론_

위에서 인공 뉴런의 출력값을 변경시키는 함수를 **활성화 함수(Activation Function)**라고 한다. 앞서 언급된 소프트맥스 함수나 시그모이드 함수 등의 함수 또한 활성화 함수 중 하나이며, 목적에 맞는 활성화 함수를 사용한다. 딥러닝 관점에서 로지스틱 회귀는 시그모이드 함수라는 활성화 함수를 사용하는 하나의 인공 뉴런으로 볼 수 있다. 여태 언급된 인공 뉴런을 수식으로 표현하면 아래와 같다.

$$ f(\sum_{i}^n w_i x_i + b) $$

위의 수식에서 f는 활성화 함수를 말하며, 인공 신경망을 구성하는 인공뉴런과 퍼셉트론의 차이는 오직 활성화 함수이다. 퍼셉트론에서는 활성화 함수로 계단 함수를 사용한 것이라고 볼 수 있다. 용어 정리를 해보자면 퍼셉트론은 인공 뉴런의 한 종류이고, 인공 신경망은 인공 뉴런들로 이루어진다고 볼 수 있다.

## **다층 퍼셉트론(Multi-Layer Perceptron, MLP)**

위에서 언급한 퍼셉트론을 단층 퍼셉트론(Single-Layer Perceptron)이라고 한다. **다층 퍼셉트론**이란 단층 퍼셉트론이 연속적으로 층을 구성한 것이다. 단층 퍼셉트론은 입력층과 출력층만 존재하지만, 다층 퍼셉트론은 입력층과 출력층 사이에 다른 층들이 존재하는데, 이 층들을 **은닉층(hidden layer)**이라고 한다. 은닉층이 2개 이상인 인경망을 **심층 신경망(Deep Neural Network, DNN)**이라고 부른다. 심층 신경망이 적절한 가중치를 찾아가는 과정이 학습 또는 훈련이고, 머신 러닝에서 사용하는 손실 함수와 옵티마이저를 사용한다. 그리고 학습 시키는 인공 신경망이 심층 신경망일 경우 **딥 러닝(Deep Learning)**이라고 부른다.

![Desktop View](/assets/img/입은층.png)
_다층 퍼셉트론_

## **인공 신경망(Artificail Neural Network)**

다음은 인공 신경망의 기본적인 내용들이다.

### 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)

**피드 포워드 신경망**이란 연산이 오직 입력층에서 출력층 방향으로 진행되는 신경망을 말한다. 위 사진의 다층 퍼셉트론은 피드 포워드 신경망이다. 다른 종류로는 은닉층의 출력값이 다시 은닉층의 입력값으로 사용되는 신경망이 있는데, 이런 신경망을 순환 신경망이라고 하고 다음에 다룰 것이다.

### 전결합층(Fully-connected layer, FC, Dense layer)

신경망에서 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때, 이 층을 **전결합층** 또는 완전연결층이라고 부른다. 위 사진의 다층 퍼셉트론의 예시에서도 모든 은닉층과 출력층이 전결합층이다. 동일한 의미로 밀집층(Dense layer)이라고 부르기도 한다.

### 행렬 관점에서의 이해

신경망에서 입력층에서 출력층 방향으로 연산이 진행되는 과정을 **순전파(Forward Propagation)**라고 한다. 위의 피드 포워드 신경망과 헷갈릴 수 있는데 순전파는 단순히 연산이 진행되는 과정을 뜻한다. 입력층 뉴런이 3개, 출력층 뉴런이 2개인 인공신경망이 있다고 하자. 행렬 관점으로 연산을 본다면 입력값인 3차원 벡터에 3x2의 가중치 행렬이 곱해지고 2개의 편향이 더해져 2차원의 출력값을 계산하는 것이다. 이후에 출력된 2차원의 벡터에 활성화 함수를 씌우게 된다. 따라서 한 층의 가중치는 입력층의 크기 x 출력층의 크기인 행렬로 표현할 수 있고, 편향은 출력층의 크기만큼 가진다고 볼 수 있다.

행렬계산의 장점은 **병렬 연산**이다. 동시에 여러 입력을 처리할 때, 입력의 수만큼 행을 가지는 입력 행렬을 가중치, 편향과 계산하면 여러 샘플을 동시에 계산할 수 있다. 이때 매개변수의 수는 변하지 않으며 이런 연산 방식을 배치 연산이라고 부른다.

![Desktop View](/assets/img/parallel_nn.png)
_배치 연산_

### 활성화 함수(Activation Function)

활성화 함수를 사용하는 이유는 복잡도를 올리기 위함이다. 현실 세계의 문제들은 명료하지 않다. 우린 이러한 복잡하고 얽힌 문제들을 풀어나가기 위해 인공 신경망을 연구하고 있는 것이다. 그렇기에 인공 신경망이 여러 층과 많은 가중치와 편향을 가지며 각 층마다 연산과 함수를 적용하는 복잡한 구조를 띄게 된다. 만약 이때 적용되는 함수가 선형 함수라면 이 복잡한 구조가 단 하나의 층으로 표현될 수 있기에 의미가 옅어진다.

활성화 함수가 선형 함수라면 발생하는 문제는 다음과 같다. $$ f(x) = wx $$일 때 2개의 은닉층을 가진다고 가정해보겠다. $$ y(x) = f(f(f(x))) = w^3 x $$이고 $$ w^3 $$역시 상수이므로 $$ y(x) = kx $$가 된다. 따라서 활성화 함수는 비선형 함수여야만 하고 이러한 활성화 함수와 복잡한 층 구조를 통해 신경망의 표현성이 높아지게 된다. 다음은 대표적인 활성화 함수들이다.

+ 계단(step) 함수 - 거의 사용되지 않지만 퍼셉트론을 통해 인공 신경망을 처음 배울 때 접한다.

+ 시그모이드(sigmoid) 함수 - 그래프의 양끝으로 갈수록 완만해져 미분값이 작아진다. 이는 학습 시에 가중치와 편향의 업데이트가 제대로 되지 않는 원인이 되어 은닉층에서는 사용이 지양되고 이진 분류 시에 출력층에서 주로 사용된다.

+ 하이퍼볼릭탄젠트(tanh) 함수 - 시그모이드 함수와 비슷하나 -1부터 1사이의 출력값을 가진다. 시그모이드 함수와 같은 문제가 발생하지만 은닉층에서 시그모이드 함수보단 선호된다.

![Desktop View](/assets/img/하이퍼볼릭탄젠트.png)
_tanh function_

+ 렐루(ReLU) 함수 - 은닉층에서 가장 인기있는 함수이며 수식은 $$ f(x) = max(0, x) $$이다. 다만 입력값이 음수일 때 미분값이 0이 된다. 이때 이 뉴런은 회생하는 것이 힘들어 이런 문제를 죽은 렐루(dying ReLU)라고 부른다.

![Desktop View](/assets/img/렐루함수.png)
_ReLU function_

+ 리키 렐루(Leaky ReLU) - 죽은 렐루를 보완하기 위한 ReLU함수의 다양한 변형 함수 중 하나이다. 수식은 $$ f(x) = max(ax, x) $$로, a는 Leaky의 정도를 결정하는 하이퍼파라미터이다. 일반적으로는 0.01의 값을 가지며 덕분에 입력값이 음수라도 기울기가 0이 되지 않아 죽은 렐루 문제가 발생하지 않는다.

+ 소프트맥스(softmax) 함수 - 은닉층에서는 보통 ReLU 또는 ReLU의 변형 함수를 사용하는 것이 일반적이다. 소프트맥스 함수는 시그모이드 함수와 같이 출력층에서 주로 사용되며, 세 가지 이상의 선택지 중 하나를 고르는 다중 클래스 분류 문제에 주로 사용된다.

## **학습 방법**

학습 방법은 머신러닝과 동일하다. 입력값이 가중치, 편향과 계산되는데 단지 층 구조로 이루어져 있어 연산 횟수가 많다. 순전파로 연산이 진행되고 손실 함수를 통해 오차를 계산하고 옵티마이저를 통해 매개변수를 조정한다.

### 손실 함수(Loss Function)

기존 머신러닝의 개념과 같다. 예측값과 실제값의 차이를 수치화해주는 함수이며 문제에 따라 적절한 손실 함수를 설정하는 것이 중요하다. 다음은 다양한 손실함수들이다.

1. MSE(Mean Squared Error) - 선형 회귀에 주로 쓰이는 손실 함수로 평균 제곱 오차이다. 연속형 변수를 예측할 때 사용된다. 딥러닝 자연어 처리는 대부분 분류 문제이므로 MSE보다는 크로스 엔트로피 함수들을 주로 사용한다.
2. 이진 크로스 엔트로피(Binary Cross-Entropy) - 시그모이드 함수를 사용하는 이진 분류 문제에 주로 사용하는 손실함수이다.
3. 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy) - 소프트맥수 함수를 사용하는 다중 클래스 분류 문제에 주로 사용된다.

그외에 다양한 손실함수들이 존재한다.

### 배치 크기(Batch Size)에 따른 경사 하강법

**배치(Batch)**란 가중치와 편향같은 매개 변수를 업데이트하기 위해 사용하는 데이터의 크기를 말한다. 전체 데이터를 한바퀴 다 돌린 후 업데이트할 수도 있고, 특정한 크기의 데이터 마다 업데이트를 할 수도 있다.

1. 배치 경사 하강법(Batch Gradient Descent) - 가장 기본적인 경사 하강법으로 오차를 구할 때 전체 데이터를 고려한다. 전체 데이터에 대한 한 번의 훈련 횟수를 1 epoch라고 하는데, 배치 경사 하강법은 1 epoch 마다 한 번 매개변수를 업데이트한다. 시간이 오래걸리며, 큰 메모리를 요구한다는 단점이 있다.

2. 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD) - 매개변수 업데이트를 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서 계산하는 방법이다. 빠른 계산과 적은 메모리 요구가 장점이다. 다만 변경폭이 불안정하고 정확도가 낮을 수도 있다는 점이 단점이다.

3. 미니 배치 경사 하강법(Mini-Batch Gradient Descent) - 배치 크기를 지정하여 해당 데이터 개수만큼 계산하여 매개 변수를 업데이트한다. 전체를 계산하는 것보다 빠르고 SGD보다 안정적이라 가장 많이 사용되는 경사 하강법이다. 배치크기는 주로 2의 n제곱으로 설정한다.

### 옵티마이저(Optimizer)

다음은 다양한 옵티마이저들이다.

1. 모멘텀(Momentum) - 경사하강법에 관성을 더 해주는 것이다. 경사하강법에서 계산된 접서의 기울기에 그 전 시점의 접선의 기울기값을 일정한 비율만큼 반영하여 로컬 미니멈에서 탈출하고 글로벌 미니멈을 향할 수 있다.

2. 아다그리드(Adagrad) - 각 매개변수에 서로 다른 학습률을 적용시킨다. 변화가 많은 매개변수는 작은 학습률을, 변화가 적은 매개변수는 높은 학습률을 설정한다.

3. 알엠에스프롭(RMSprop) - 아다그리드는 학습을 진행할수록 학습률이 지나치게 떨어진다는 단점이 있다. 이를 보완한 옵티마이저가 알엠에스프롭이다.

4. 아담(Adam) - 알엠에스프롭과 모멘텀을 합친 듯한 방법이다. 방향과 학습률을 모두 잡기 위한 방법이다.

이외에도 다양한 옵티마이저들이 사용될 수 있다.

### 역전파(BackPropagation)

앞서 입력층부터 출력층까지 계산하는 순전파를 언급하였다. 이후에 오차를 계산하고 경사 하강법을 통해 가중치를 업데이트하는 과정은 출력층부터 입력층으로 진행되어 이 과정을 **역전파**라고 부른다. 미분의 연쇄법칙을 사용하여 학습률만큼 반영하여 업데이트한다. 계산 예시는 생략하려고 한다.

**에포크(epoch)**란 전체 데이터에 대해 순전파와 역전파가 끝난 상태이다. **이터레이션(iteration)**이란 에포크를 배치 크기로 나눠준 것이다. 예를 들어 전체 데이터의 크기가 1000이고 배치크기가 200이라면 이터레이션은 5이다. 한 번의 에포크를 끝내기 위해 필요한 배치의 수라고 볼 수 있으며 **스텝(Step)**이라고 부르기도 한다.

## **과적합(Overfitting)을 막는 방법**

인공 신경망이 학습하다 보면 훈련 데이터에 대해 과도하게 적합해지는 현상이 발생한다. 이는 모델의 성능을 떨어트리는 주요 이슈이며, 훈련 데이터를 과도하게 암기하여 중요하지 않은 노이즈까지 학습한 상태라고 볼 수 있다. 과적합을 막는 방법들은 다음과 같다.

### 데이터의 규모 늘리기

학습 데이터의 양이 부족한 경우, 데이터의 패턴이나 노이즈까지 학습하게 되어 과적합 현상이 발생할 확률이 높아진다. 따라서 데이터의 양이 증가한다면 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 없다. 만약 데이터를 늘리기 어려울 경우에는 기존의 데이터를 의도적으로 변형한 후 추가하기도 하는데 이 방법을 **데이터 증강 또는 증식(Data Augmentation)**이라고 한다. 이미지의 경우에는 회전이나 반전, 노이즈 추가, 일부분을 수정하는 등의 방식을 채용하고, 텍스트 데이터의 경우에는 번역 후 재번역을 하여 새로운 데이터를 생성하는 **역변역(Back Translation)** 등의 방법이 있다.

### 모델의 복잡도 줄이기

모델이 복잡할수록 과적합 현상이 발생할 확률이 높다. 복잡한 구조는 표현성이 높아 더 섬세하게 학습하기 때문이다. 복잡도는 은닉층의 수, 매개변수의 수 등으로 결정된다. 과적합 현상이 발생하였을 때, 복잡도를 줄이는 것이 하나의 해결책이 될 수도 있다. 또한 모델에서 매개변수의 수들을 모델의 **수용력(capacity)**라고 하기도 한다.

### 가중치 규제(Regularization)

**가중치 규제**는 복잡한 모델을 간단하게 만드는 하나의 방법이다. 많은 수의 매개변수를 가지고 있는 복잡한 모델에 가중치 규제를 적용함으로써 모델을 비교적 간단하게 만들 수 있다. 다음 가중치 규제의 대표적인 두가지 방법이다.

+ L1 규제(노름) - 가중치 w들의 절대값의 합($$ \lambda w $$)을 비용함수에 추가
+ L2 규제(노름) - 가중치 w들의 제곱합($$ 1/2 \lambda w $$)을 비용함수에 추가

이때 $$ \lambda $$는 규제의 강도를 나타내는 하이퍼파라미터이며, $$ \lambda $$가 커질수록 모델은 학습을 진행하면서 학습 데이터에 적합한 매개변수를 찾는 것보다 추가된 항들을 작게 유지하는 것을 우선시하게 된다. 2가지 규제 모두 비용함수를 최소화하기 위해 적합한 가중치와 편향을 찾는 동시에 가중치가 작아져야 한다. 이렇게 되면 가중치의 값들이 0 또는 0에 가까지 작아지게 되면서 어떤 특성들은 거의 사용되지 않기도 한다. L1 규제를 적용하게 되면 어떤 가중치는 0이 되기도 하는데 이는 0이 된 가중치가 모델의 결과에 영향을 주지 못하는 특성임을 의미한다. 하지만 L2 규제는 제곱합을 최소화하므로 가중치가 0이 되기보단 0에 가까워지는 경향을 보이는데 이러한 이유로 L1 규제는 어떤 특성들이 모델에 영향을 주는지 판단할 때 유용하고 성능적으로는 L2 규제가 더 잘 작동한다고 한다. 인공신경망에서는 L2 규제를 가중치 감쇠(decay)라고 부르기도 한다.

정규화라고 해석할수도 있지만, Normalization과 혼동의 여지가 있어서 규제 또는 정형화라고 표현하였다.

### 드롭아웃(Dropout)

**드롭아웃**이란 학습 시에 신경망의 일부를 사용하지 않는 방법이다. 만약 드롭아웃 비율을 0.5로 설정한다면 학습 마다 랜덤으로 절반의 뉴런을 사용하지 않게 된다. 학습 시에만 사용하고 예측 시에는 사용하지 않는 것이 일반적이다. 학습 시에 인공 신경망이 특정 뉴런 또는 뉴런의 조합에 의존적이게 되는 것을 방지하고, 매번 랜덤으로 뉴런을 선택하므로 서로 다른 신경망들을 앙상블하여 사용하는 것과 같은 효과를 내어 과적합을 방지한다. 동일한 신경망으로 계속 학습한다면 학습 데이터에 지나치게 적합해질 수 있는데 드롭아웃을 이용하여 조금씩 다른 신경망이 학습하여 합쳐지는 것과 비슷하므로 데이터의 일반적인 패턴을 학습한다는 것이 장점인 것 같다.

## **기울기 소실(Gradient Vanishing)과 폭주(Exploding)**

인공 신경망을 학습하다 보면 역전파 과정에서 입력층으로 갈수록 기울기가 작아지는 현상이 발생한다. 이는 입력층에 가까운 매개변수들의 업데이트가 제대로 되지 않음을 의미하고 모델의 성능이 낮아지게 되는데 이 현상을 **기울시 소실**이라고 한다.

반대로 기울기가 점차 커지더니 매개변수들이 비정상적으로 큰 값이 되면서 발산하기도 하는데, 이를 **기울기 폭주**라고 하며, 순환신경망에서 쉽게 발생할 수 있다. 다음은 이러한 기울기 소실과 폭주를 막는 방법들이다.

### ReLU와 ReLU의 변형

앞서 시그모이드 함수를 은닉층에서 사용하는 것을 지양해야 한다고 언급했다. 이는 그래프의 모양 때문에 기울기 소실 현상이 발생하기 때문인데,  같은 이유로 하이퍼볼릭탄젠트 함수도 사용을 지양해야 한다. 따라서 기울기 소실을 막는 첫번째 방법은 은닉층에서 ReLU나 Leaky ReLU같은 ReLU의 변형 함수를 사용하는 것이다.

### 그래디언트 클리핑(Gradient Clipping)

**그래디언트 클리핑**은 기울기 값을 자르는 것을 의미한다. 기준치를 설정하고 기울기 값이 기준치를 넘어가면 넘어간 만큼을 자르게 된다. 이 방법은 순환신경망에서 유용하게 쓰인다.

### 가중치 초기화(Weight Initialization)

같은 모델을 훈련시키더라도 초기에 가중치 값에 따라 모델의 성능이 좌우되기도 한다. 적절한 가중치 초기화는 기울기 소실과 같은 문제를 완화하는 데에 도움이 된다. 다음은 초기화 방법이다.

+ 세이비어 초기화(Xavier initialization) - 시그모이드 함수나 하이퍼볼릭탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에 좋은 성능을 보인다. 이전 층의 뉴런의 개수와 다음 층의 뉴런의 개수를 고려하며, 균등 분포와 정규 분포 2가지로 초기화할 수 있다.

+ He 초기화 - 세이비어 초기화와 유사하게 정규분포와 균등 분포 두가지 경우로 나뉜다. 하지만 He 초기화는 다음 층의 뉴런 개수는 고려하지 않고, 이전 층의 뉴런만 고려한다. ReLU 계열의 함수와 사용할 경우에 좋은 성능을 보인다. 보편적인 방법은 ReLU + He 초기화 방법이다.

### 배치 정규화(Batch Normalization)

**배치 정규화**는 가중치 초기화와 같이 기울기 소실과 폭주를 예방하는 방법이다. 배치 정규화는 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다. 조금 이해하기 어려운 개념일 수 있는데 쉽게 설명해보자면 신경망을 학습할 때, 입력 데이터가 각 층을 지나면서 연산이 진행된다. 이때 가중치와 활성화 함수의 연산을 한 후, 다음 층으로 넘어갈 때 이전 층에 입력됐던 데이터는 다른 모양을 띄게 된다. 연산이 진행됐기 때문이다. 이렇게 층 별로 입력 데이터 분포가 달라지는 현상을 **내부 공변량 변화(Internal Covariate Shift)**라고 하며, 이 현상은 기울기 소실과 폭주 등 모델의 불안전성을 초래한다.

배치 정규화는 학습 때 설정된 배치 단위로 정규화하는 것을 말하며, 각 층에서 활성화 함수를 통과하기 전에 수행된다. 원활한 학습을 위해 연산하면서 점차 달라지는 그래프를 계속 일정한 모양으로 유지해주는 것이라고 보면 된다. 스케일과 시프트를 위한 두 개의 매개변수를 사용하며, 미니 배치에 대한 평균과 분산으로 정규화한 후 스케일 조정과 시프트를 통해 일정한 범위 안에서 유지되도록 한다. 테스트할 때는 해당 배치의 평균과 분산 대신 학습 시에 구해놓았던 평균과 분산을 통해 정규화한다. 자세한 수식은 생략하려고 한다.

배치 정규화를 사용하면 시그모이드나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실의 문제가 개선되며, 가중치 초기화에 덜 민감해진다. 또한 큰 학습률을 적용할 수 있어, 학습 속도를 개선한다. 또, 배치마다 평균과 표준편차를 계산 후 사용하므로 일종의 의도적인 노이즈를 추가하는 효과로 과적합을 방지할 수 있는데, 이는 드롭아웃과 비슷한 효과를 내며 드롭 아웃과 함께 사용하는 것이 이상적이다.

다만, 모델을 복잡하게 하고 연산이 추가되어 실행 시간이 느려진다. 따라서 꼭 필요한 작업인지 판단할 필요가 있다. 또한, 너무 작은 배치 크기에 대해서는 잘 작동하지 않는데 극단적인 예시로는 배치 크기가 1이라면 분산이 0이 된다. 따라서 배치 크기에 의존하는 경향이 있다. 또, 각 시점마다 다른 통계치를 가지는 순환신경망에 경우 적용이 어려워 순환 신경망의 경우에는 다른 방법을 채용하거나 부가적인 작업이 필요한 것으로 보인다.

### 층 정규화(Layer Normalization)

**층 정규화**란 같은 특성끼리 정규화를 하는 일반적인 접근에서 방향을 다르게 하여 한 샘플 안에서 정규화를 하는 것이다. 이 방법은 순환 신경망에도 적용이 수월하다. 배치 정규화가 열 방향으로 이루어 진다면 층 정규화는 행 방향으로 정규화가 이루어진다고 볼 수 있다.

크게 보면 인공 신경망의 초기 구조, 학습 방법, 딥러닝을 하면서 생기는 문제들을 막는 방법에 대해 다루어 보았다. 내용이 너무 방대하다고 생각이 들 수 있다. 다만 하나 하나의 방법에 집중하기 보다는 이 방법이 어떤 목적으로 쓰이는 지 언제 적용하는 지가 더 중요하다고 생각하고, 큰 줄기에서 부터 세세하게 들여다 보는 것이 도움이 될 거라 생각한다.