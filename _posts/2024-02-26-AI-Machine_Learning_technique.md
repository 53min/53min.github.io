---
title: "[인공지능] 머신 러닝 기법"
author: <author_id>
date: 2024-02-26 18:00:00 +09:00
categories: [AI, Machine Learning]
math: true
tag: [자연어처리, 머신 러닝, NLP, Machine Learning, 선형회귀, 로지스틱회귀, 소프트맥스 회귀, 평균 제곱 오차, 경사 하강법, 크로스 엔트로피]
toc: true
---

> 이 포스트는 [**딥 러닝을 이용한 자연어 처리 입문**](https://wikidocs.net/book/2155)을 읽고 공부 겸 정리한 글입니다.

[**머신러닝**](https://53min.github.io/posts/AI-Machine_Learning/)을 읽고 오시면 더욱 도움이 됩니다.

이 포스트는 이전 머신러닝에 대해 다루었던 포스트에서 사용된 개념을 이용하여 기본적인 기법들을 정의해보려 한다.

## 선형 회귀(Linear Regression)

집의 평수가 클수록 집값이 비싸지고, 운동을 오래할수록 몸무게가 줄어든다. 이처럼 어떤 한 변수가 다른 변수에게 영향을 주고 그 둘의 관계가 직선적일 때 선형적이라고 한다.영향을 주는 변수를 독립변수라 칭하고 x로 표현하며, 영향을 받는 변수를 종속변수라 하고 y로 표현한다. 따라서 선형회귀는 내가 예측하고 싶은 값이 있고 이 값에 영향을 주는 변수들이 이 값과 선형적일 때 사용한다. 독립변수 x가 1개일 때 단순 선형 회귀라고 하고, 2개 이상일 때 다중 선형 회귀라고 한다.

### 가설

y는 x에 영향을 받으며 선형적이다. 이 개념은 수학에서 직선의 방정식과 유사하다. 따라서 가설 역시 직선의 방정식과 동일하다. 가설은 H(x)라 표현하기도 한다.

- 단순 선형 회귀

$$ y = wx + b $$

- 다중 선형 회귀

$$ y = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$

### 평균 제곱 오차(MSE)

회귀문제의 경우 주로 비용함수로 **평균 제곱 오차(Mean Squared Error, MSE)**를 사용한다. 가중치와 편향을 설정하고 데이터를 대입하여 오차를 구해야한다. 오차는 주어진 데이터의 가설에 대입한 예측값 H(x)과 실제값 y의 차이를 말한다. 데이터가 존재하는 각 지점에서 오차가 발생할텐데 기본적으로는 모든 오차를 종합하여 하나의 오차로 표현하고 이 오차를 줄여가며 가중치와 편향을 업데이트한다. 

하지만 만약 오차가 음수도 존재하고 양수도 존재한다면 절대적인 오차의 크기를 구할 수 없기 때문에 오차를 제곱해서 더하고 이를 평균으로 나누는 방법이 평균 제곱 오차이다. 즉 모든 점에서 오차가 커지면 평균 제곱 오차도 커진다.비용함수를 가중치와 편향에 의한 평균 제곱 오차라고 했을 때 수식으로 표현하면 아래와 같다.

$$ cost(w, b) = {1 \over n}\sum_{i=1}^n [y^{(i)} - H(x^{(i)})]^2 $$

### 경사하강법(Gradient Descent)

기법마다 꼭 정해진 옵티마이저 알고리즘은 없지만 이번 포스트에서는 가장 기본적인 알고리즘인 **경사 하강법**에 대해 다루겠다. 경사하강법에 대해 알기 전에 cost와 w,b의 관계에 대해 알아보자. 내가 세운 가설에서 예측값과 실제값의 차이를 가장 적게하는 w와 b가 존재할 것이다. 여기서 cost는 비용함수의 값을 의미하고, 이해가 쉽도록 가중치만 고려한다고 하자. cost가 최소값을 가지는 w가 있다고 할 때, 이 가중치 값에서 가중치가 더 커져도 cost가 증가할 것이고, 가중치가 더 작아져도 cost가 증가할 것이다. 이러한 관계를 그래프로 그려본다면 아래와 같다.

![Desktop View](/assets/img/선형회귀1.jpg)
_w와 cost의 관계_

물론 실제 그래프는 위의 그래프보다 더욱 복잡한 높은 차수의 함수 그래프로 표현될 것이다. 하지만 일단 학습이란 결국 위 그래프에서 아래 최솟값을 목표로 w를 업데이트 해나가는 것이다.

경사하강법이란 현재 가중치에서 가지는 cost의 순간 변화율 또는 접선의 기울기를 이용하여 w를 업데이트 하는 것이다. 미적분을 배웠다면 최소값에서 접선의 기울기가 0이라고 알고 있을 것이다. 현재 접선의 기울기가 양수라면 w를 감소시키고, 접선의 기울기가 음수라면 w를 증가시켜 접선의 기울기가 0이 되는 w를 찾는 방법이 경사하강법이다. w를 업데이트하는 식은 아래와 같다.

$$ w := w - a {\partial \over \partial w} cost(w) $$

위의 식을 해석하자면 현재 가중치에서 a와 현재 가중치에서의 접선의 기울기의 곱을 빼서 업데이트한다는 것이다. 여기서 a는 **학습률**을 의미하는데 하이퍼파라미터의 종류로 업데이트할 때 얼마나 크게 변경할 지를 결정하는 0과 1 사이의 수이다. 학습률을 너무 적게 설정한다면 학습이 오래걸려 효율이 떨어지고, 너무 크게 설정하면 최소값을 지나쳐 발산될 수도 있다. 따라서 적절한 학습률을 설정하는 것이 중요하다.  

![Desktop View](/assets/img/선형회귀5.jpg)
_학습률이 너무 크면 발생하는 일_

## 로지스틱 회귀(Logistic Regression)

로지스틱 회귀란 이름은 회귀지만 사실은 분류 문제에 사용된다. 분류 중에서도 두가지 선택지 중에 분류하는 것을 **이진 분류**라고 하는데, 이 문제에 적용되는 대표적인 알고리즘이 로지스틱 회귀이다. 예를 들어, 60점 이상은 합격, 미만은 불합격을 판정하는 경우가 이진 분류의 예시라고 할 수 있다.

### 가설

로지스틱 회귀는 선형회귀와 다르게 둘 중 하나의 선택지를 고르는 문제에 사용되므로 직선 형태의 함수를 가설로 설정하는 것은 옳지 않다. 보통 두개의 선택지 중 하나를 0, 다른 하나를 1로 설정하므로 직선 형태의 그래프보다는 0과 1 사이에 S자 형태의 그래프를 표현할 수 있는 함수가 더 적합하다.

![Desktop View](/assets/img/로지스틱회귀.png)
_60점 이상은 합격임을 나타내는 그래프(합격은 1, 불합격은 0)_

그래프에 대입했을 때, 0.5 보다 크면 1, 0.5 보다 작으면 0을 예측했다고 판단하여 분류한다. 이러한 요건을 충족하는 함수가 바로 **시그모이드 함수(Sigmoid function)**이다. 시그모이드 함수를 이용한 가설의 수식과 그래프 모양은 아래와 같다.

$$ H(x) = {1 \over {1 + e^{-(wx + b)}}} = sigmoid(wx + b) = \sigma(wx + b) $$

![Desktop View](/assets/img/시그모이드그래프.png)
_시그모이드 함수_

### 비용 함수

로지스틱 회귀에서 옵티마이저로 경사하강법 알고리즘을 주로 사용하지만 비용 함수는 선형회귀와 다른 함수를 사용한다. 이유는 가중치가 로컬 미니멈에 도달할 확률이 높기 때문이다. 로컬 미니멈이란 함수에서 보통 여러개의 극소값을 가질 수 있는데 이 극소값 중 가장 작은 극소값이 아닌 그래프의 특정 구역에서의 극소값을 말한다. 함수 전체에서 가장 작은 최소값은 글로벌 미니멈이라 부른다. 따라서 로지스틱 회귀에서는 다른 비용함수를 주로 사용하는데, 이때 사용되는 함수가 **크로스 엔트로피(Cross Entropy)함수**이다. 함수의 수식은 아래와 같다.

$$ cost(w, b) = -{1 \over n}\sum_{i=1}^n[y^{(i)}logH(x^{(i)}) + (1 - y^{(i)})log(1 - H(x^{(i)}))] $$

시그모이드 함수는 0과 1 사이의 값을 반환하므로, 실제값이 0일 때 1에 가까워지면 오차가 커지고, 실제값이 1일 때 0에 가까워지면 오차가 커져야 한다. 이를 반영한 것이 위의 크로스 엔트로피 함수이다.

 시그마 안의 식에서 만약 실제값 y가 1이고 예측값 H(x)가 1이라면 뒤의 식이 0이 되고 앞의 식의 로그도 0이 되어 cost는 0이 되고 예측값이 0에 수렴하면 cost가 무한으로 발산한다. 두개의 식이 + 로 연결되어 있는데 실제 값에 따라 둘 중 하나의 식은 0이 되어 실제값에 따라 적절한 오차값을 계산할 수 있다.

## 소프트맥스 회귀(Softmax Regression)

앞서 로지스틱 회귀가 두가지 선택지 중 하나를 선택하는 이진 분류에 적용된다고 하였다. 소프트맥스 회귀는 3개 이상의 선택지 중 하나를 고르는 **다중 클래스 분류**에 적용되는 알고리즘이다. 큰 틀은 로지스틱 회귀와 비슷하며 로지스틱 회귀가 소프트맥스 회귀의 특수한 경우라고 생각하면 쉽다.

### 가설

첫번째 접근법으로는 각 선택지에 대해 시그모이드 함수를 적용해볼 수 있다. 그렇다면 각 선택지에 대해 정답인 확률을 얻을 수 있다. 하지만 각 선택지에 대한 확률의 전체 확률이 1이 되도록 바꾼다면 더 편할 것이다. 가장 높은 확률 값을 가진 선택지를 고를 수 있기 때문이다. 이 부분에 적용할 수 있는 함수가 바로 **소프트맥스 함수**이다.

선택지가 k라고 할 때 소프트맥수 함수는 k차원의 벡터를 입력받아 각 선택지에 대한 확률을 반환한다. 
입력받은 벡터를 z라고 할때, 각 선택지에 대한 확률은 다음과 같이 계산한다.

$$ p_i = {e^{z_i} \over {\sum_{j=1}^k e^{z_j}}} $$

이렇게 비율로 계산하면 각 선택지에 대한 확률의 총 합이 1로 표현된다. 선택지가 3개이고 3차원의 벡터 $$ z = [z_1, z_2, z_3] $$ 를 입력했을 때 소프트맥스 함수는 다음과 같이 반환한다.

$$ softmax(z) = [{e^{z_1} \over {\sum_{j=1}^3 e^{z_j}}} {e^{z_2} \over {\sum_{j=1}^3 e^{z_j}}} {e^{z_3} \over {\sum_{j=1}^3 e^{z_j}}}] = [p_1, p_2, p_3] = 예측값 $$

그렇다면 독립변수들을 선택지에 맞는 차원의 벡터로 변환하는 것에 대한 궁금증이 생긴다. 선형변환이라고 하는데 이 선형변환을 할 때 가중치와 편향이 적용된다. 만약 독립변수가 4개이고 선택지가 3개라면 3x4 행렬을 독립변수에 곱한다. 독립변수를 4x1 행렬로 두고 계산하면 행렬곱에 따라 3차원의 벡터를 얻게되고 이 벡터가 소프트맥수 함수에 입력된다. 이때 4x3 행렬에 총 12개의 값이 모두 다른 가중치이며 역시 비용함수와 옵티마이저를 통해 오차를 줄이는 방향으로 변경된다.

### 비용함수

앞서 로지스틱 회귀가 소프트맥스 회귀의 특수한 경우라고 언급하였다. 소프트맥스 회귀에서 주로 사용되는 비용함수는 로지스틱 회귀에서 사용한 비용함수의 일반화라고 생각하면 된다.

먼저 오차를 구하는 방법이다. 앞서 로지스틱 회귀와는 다르게 3개 이상의 선택지가 존재하며, 오차는 실제값과 예측값 사이에 차이로 구해야한다. 선택지가 k라고 하면 소프트맥스 함수의 반환값이 각 선택지가 정답일 확률을 가진 k차원의 벡터로 나올 것이고, 실제값은 k개의 선택지 중 하나일 것이다. 둘의 양식을 맞추기 위하여 실제 값을 예측값과 같은 k차원의 벡터로 구성하고, 정답인 선택지의 인덱스는 1, 나머지 정답이 아닌 선택지의 인덱스는 0을 가지도록 원-핫 인코딩을 한다. 이후 두 벡터의 오차를 주로 크로스 엔트로피 함수를 이용하여 계산한 후, 오차로부터 가중치와 편향을 업데이트한다.

클래스가 순서의 의미를 가지고 있으면 원-핫 인코딩이 아닌 정수 인코딩을 사용하기도 하는데 내용이 방대해질 수 있어 다루지 않겠다.

아래의 수식은 소프트맥스 회귀에서 비용함수로 주로 사용하는 크로스 엔트로피 함수이다. k는 선택지(클래스)의 개수이며 y는 실제값, p는 데이터가 j번 째 클래스일 확률을 나타낸다.

$$ cost = -\sum_{j=1}^k y_j log(p_j) $$

실제값은 원-핫 벡터로 구성되어 있으므로 정답인 인덱스만 1이고 정답이 아닌 인덱스는 모두 0이다. 따라서 정답이 아닌 인덱스에 대해 계산할 때는 y가 0이므로 계산값이 모두 0이 되고 정답인 인덱스에 대해서만 오차가 계산된다. 만약 정답인 인덱스에 대하여 정확하게 예측한 경우 -1log(1) = 0 이므로 오차는 0이 된다. k를 2로 설정한다면 로지스틱 회귀에 사용한 비용함수가 되며 이 비용함수를 바이너리 크로스 엔트로피(binary cross entropy)함수라 부른다. 위 함수를 전체 데이터에 대하여 평균을 구한다면 아래와 같다.

$$ cost = -{1 \over n}\sum_{i=1}^n\sum_{j=1}^k y_j^{(i)}log(p_j^{(i)}) $$

이번에는 머신러닝에서 기본이 되는 기법들에 대해 다루었다. 처음에는 어떤 기법에 어떤 함수와 옵티마이저가 쓰이는구나 했다. 하지만 결국 정해진 건 없고 각 비용함수와 옵티마이저가 어떤 식으로 작동하는 지를 알고 적재적소에 적용할 수 있는 능력이 더 중요하다고 느꼈다.