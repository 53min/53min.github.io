---
title: "[자연어 처리] 텍스트 전처리"
author: <author_id>
date: 2024-01-31 18:00:00 +09:00
categories: [NLP, Preprocessing]
tag: [자연어처리, 전처리, NLP, Preprocessing, 토큰화, 정제, 정규화, 어간 추출, 표제어 추출, 정규 표현식, 정수 인코딩, 패딩, 원-핫 인코딩, 한국어 전처리]
toc: true
---

> 이 포스트는 [**딥 러닝을 이용한 자연어 처리 입문**](https://wikidocs.net/book/2155)을 읽고 공부 겸 정리한 글입니다.

## 자연어처리

자연어(natural language)란 우리가 일상에서 사용하는 언어이다. 따라서 자연어 처리(natural language processing)란 우리가 사용하는 언어를 다양한 방식으로 컴퓨터가 처리하는 과정이다.

언어에 관련된 만큼 다양한 분야에 적용할 수 있고 발전 가능성도 무궁무진하여 IT분야에서 굉장히 중요 기술로 떠올랐다. 예시로는 Chat-GPT같은 챗봇과 번역, 음성 인식, 텍스트 분류 등이 있다.

여러 자연어 처리 기법을 제공하는 라이브러리로는 영어는 NLTK와 keras, 한국어는 KoNLPy 등이 있다.

## 전처리

여러 분야에 적용되는 만큼 자연어 처리 기법은 목적에 따라 다양하게 존재한다. 따라서 특정 기법을 사용하기 전에 사용할 자연어를 그 기법에 맞게 사전에 전처리를 해주어야한다. 전처리가 어떤 식으로 진행되냐에 따라 결과가 눈에 띄게 달라지기도 한다.
아래는 여러가지 전처리 방법을 보기 편하게 정리하였다. 필자가 배운 지식(대학교 학부생 2학년)으로는 위에서 아래 순서로 작업을 진행한다고 배웠는데 필요에 따라 다르게 진행할 수도 있을 것 같다. 아래 방법들은 기본적인 방법들이며 본 내용에서 다루지 않은 다른 방법들도 있다.

### 전처리 방법

- 토큰화
  + 단어 토큰화
  + 문장 토큰화
  + 형태소 토큰화
- 정제
  + 불용어 제거
- 정규화
  + 대, 소문자 통합
  + 정규 표현식 이용
  + 표제어 추출
  + 어간 추출
- 정수 인코딩
- 패딩
- 원-핫 인코딩

지금부터는 각 방법을 조금 더 자세하게 이야기해 보고자 한다.

## 1. 토큰화

먼저 자연어 처리에서 사용되는 자연어들의 집합을 코퍼스(corpus)라고 한다.
전처리 단계에서 초반에 진행되는 주요 작업은 토큰화이다. 토큰화(tokenization)란 코퍼스에서 목적에 맞게 토큰을 분류하는 것인데, 쉽게 말하면 하나의 큰 집합을 적절한 크기로 나누는 것이라고 보면 된다. 단어 토큰화는 단어 단위로 분리하고, 문장 토큰화는 문장 단위로 분리한다.

> 예를 들어, "I love you!"의 단어 토큰화는 "I", "love", "you"이다.

### 의외의 문제점들
이렇게만 본다면 단순히 띄어쓰기와 구두점 기준으로 분리하면 되지 않나?라고 생각하면 쉽지만 의외의 문제점이 발생기도한다.
눈썰미가 있는 사람이라면 알겠지만 위의 예시에서 "!"가 빠진 채로 토큰화가 진행되었다. 하지만 "ph.D"같은 단어도 있기에 무작정 특수문자를 기준으로 토큰화를 진행해선 안된다.

이와 같이 특수문자에 대한 기준도 있지만 언어의 특징에 따른 문제도 있다.
영어를 예시로 들자면 "Don't"와 "He's" 같은 "'"가 들어간 단어이다. 이러한 단어를 토큰화하는 것은 여러 방법이 있지만 선택은 사용자의 몫이다. 또한 "New York"같은 띄어쓰기가 포함된 한 단어도 있기에 무작정 띄어쓰기를 기준으로 토큰화를 진행해도 안된다.
한국어의 경우에는 더욱 복잡하다. 영어에 비해 띄어쓰기도 잘 지켜지지 않고, 같은 단어에 조사만 다른게 붙어도 의미는 같지만 모양이 전혀 다르기 때문에 훨씬 번거롭다. 따라서 보통 한국어인 경우에는 형태소를 기준으로 토큰화한다고 한다.

그렇다면 많은 양의 코퍼스를 사용자가 하나하나 토큰화 해야 하는가? 그건 아니다. 이러한 문제점들을 고려한 라이브러리가 많이 존재한다. 하지만 각각 기준이 조금씩 다르므로 여러 개의 선택지 중 본인의 목적에 맞는 라이브러리를 사용하면 된다.

## 2. 정제

정제란 코퍼스에서 불필요한 단어를 제거하는 작업을 뜻한다. 노이즈 데이터(불용어)란 단순히 의미를 가지고 있지 않은 단어를 말하기도 하지만, 분석에 도움이 되지 않는 단어를 뜻하기도 하여 본인의 목적에 따라 다를 수 있다. 정제 작업은 보통 토큰화 전, 후로 진행되며, 완벽한 정제는 어려워 어느정도 기준만 넘으면 이정도면 됐다라고 생각하기도 한다.

### 불용어 제거
불필요한 단어 제거의 첫번째 방법은 불용어 제거이다. 불용어의 예시라고 한다면 영어에서는 I, me, over같은 문장에 필수적이지만 딱히 의미 분석에는 도움이 되지 않는 단어들이다. 한국어에서는 주로 조사나 접속사가 불용어에 속한다.
이러한 불용어들도 위의 라이브러리들에서 자동으로 제거하는 방법을 제공한다고 한다.

### 등장 빈도가 적은 단어 제거
> 왜 등장 빈도가 적으면 도움이 되지 않을까?라는 생각이 든다면 스팸 메일 분류기를 만든다고 생각해보자. 10000개의 메일을 분석하는데 정상 메일에 주로 있는 단어와 스팸 메일에 주로 있는 단어가 있을 것이다. 만약에 10000개 중 2번만 등장한 단어가 있다면 이 단어는 분류에 도움이 되지 않을 것이다. 

### 길이가 짧은 단어 제거
영어권에서는 길이가 짧은 단어가 주로 불용어에 해당되는 경우가 많아 2~3글자 이내의 단어를 전부 제거하기도 한다. 평균 단어의 길이가 2~3인 한국어에는 적용하기 어려운 방법으로 보인다.

## 3. 정규화

정규화란 같은 의미지만 표현 방법이 다른 단어들을 통일된 단어로 만들어주는 작업이다.
예를 들어, "US"와 "USA"는 같은 의미이므로 한 단어로 통일할 수 있다.

### 대, 소문자 통합
첫번째 정규화 방법으로는 대, 소문자 통합이다. 한국어에는 적용되지 않지만 영어에서는 문장 첫 단어 첫 글자가 대문자로 표기되므로 단어가 중복하여 존재하면 서로 다른 단어로 인식하여 자연어 처리의 효율이 떨어질 수 있다. 하지만 "US"와 "us"는 서로 다르고 사람 이름이나 회사 이름같은 고유명사도 있기에 무작정 대, 소문자를 둘 중 하나로 통일하는 것은 좋지 않다.

### 규칙에 기반한 표기가 다른 단어 통합
두번째 방법이다. 위의 언급한 "US"와 "USA"가 이 방법에 해당한다. 이러한 통합을 도와줄 수 있는 문법적인 방법으로는 어간 추출과 표제어 추출이 있다. 두 방법 모두 생김새만 다른 단어를 하나의 단어로 일반화하여 코퍼스 내에 단어 수를 줄인다는 목적이다.  

표제어란 "기본 사전형 단어" 정도의 의미를 가진다. 표제어 추출은 어떤 단어를 그 단어의 표제어로 통합하는 것이다. 예를 들어 is, are, am의 표제어는 be이다.

형태소란 "의미를 가진 가장 작은 단위"이다. 또한 형태소에는 어간과 접사가 있는데 어간은 단어의 핵심의미, 접사는 단어의 추가적인 의미이다. 예를 들어 cats 라는 단어는 어간인 cat과 접사인 s로 이루어져 있다. 한국어에서 용언은 어간과 접사가 아닌 어간과 어미로 이루어져 있다. 어미에 따라 어간이 변하기도(불규칙활용) 안변하기도(규칙활용)하여 좀 더 복잡하다.

따라서 어간 추출은 단어에서 어간을 분리해내는 것이고, 표제어 추출은 단어의 표제어를 찾는 것이다.

### 정규 표현식 활용

정규표현식이란 의미를 가진 특수문자를 문법으로 사용하여 텍스트를 특수문자들로 표현하는 방식이다. 정규표현식을 이용하면 긴 텍스트를 짧은 특수문자들로 표현하며 규칙성이 있다면 더욱 압축이 가능하다. 파이썬에서는 re라는 모듈로 제공하며, 내용이 너무 방대하고 본 주제에서 벗어날 수 있기에 정규표현식 자체를 더 다루진 않겠다. 정규표현식은 주로 패턴과 규칙을 이용하기에 이를 이용하면 정규화뿐만 아니라 다양한 방식으로 전처리가 가능하다. 

## 4. 정수 인코딩

위의 작업들까지 완료했다면 사실상 어려운 작업은 거의 끝났다고 보면 된다. 컴퓨터는 숫자 처리에 강하다. 따라서 텍스트를 숫자로 바꾸는 것이 컴퓨터 입장에서 훨씬 좋을 것이다. 텍스트를 숫자로 바꾸는 여러가지 기법이 있는데, 정수 인코딩은 그러한 기법들을 본격적으로 적용하기 전 첫 단계로 사용되곤 한다. 작업 자체는 간단하다. 그냥 토큰화가 끝난 각 토큰들에 정수를 맵핑(mapping) 해주면 끝이다. 

> 예를 들어, I love you라는 문장을 토큰화하고 간단하게 정수 인코딩을 하면 1 2 3이다.

이 책에서는 코퍼스 내에서 토큰을 빈도순으로 단어를 정렬하고 앞에서부터 정수 인코딩을 진행한다. 빈도수가 낮은 토큰은 의미를 가지지 않을 가능성이 많기 때문이다. 따라서 기준을 정하여 특정 빈도수 이상의 토큰만 정수를 맵핑하여 토큰 집합에 저장하고 기준을 만족하지 않은 토큰은 다 똑같은 하나의 정수를 맵핑한다. 기준을 만족하지 못한 토큰은 단어집합에 "OOV(out-of-vocabulary)"를 만들어 OOV의 정수로 맵핑한다.

## 5. 패딩

자연어 처리를 진행하다보면 각 문장(또는 문서)이 서로 다른 길이를 가질 것이다. 기계는 동일한 길이를 가진 묶음을 하나의 행렬로 처리할 수 있기에 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있다. 이러한 작업을 패딩(padding)이라 한다. 이때 항상 최고 길이로의 통일은 비효율적이므로 적당한 길이를 정하여 부족한 길이는 채우고 남는 길이는 자른다. 이따 자르고 채우는 기준은 앞에서부터, 뒤에서부터 2가지가 있으므로 필요에 따라 선택하면 된다.

> 예를 들어 I love you 를 길이 5인 문장으로 패딩한다면 1 2 3 0 0 이다.

이때, 정수 인코딩을 진행했던 토큰 집합에 패딩을 맵핑해야하며 위 예시같이 0으로 패딩을 진행하는 것은 제로 패딩이라 한다. 0으로 패딩하는 것이 관례이지만 꼭 0일 필요는 없다.

## 6. 원-핫 인코딩

정수 인코딩이 텍스트를 정수로 바꾸는 것이라면 원-핫 인코딩은 텍스트를 벡터로 바꾸는 것이다. 하지만 단순히 벡터가 아닌 특별한 벡터로 바꾸는데 이 벡터는 오로지 하나의 1과 나머지 전부가 0으로 구성되어 있는 벡터로 원-핫 벡터라고 부른다. 표현하고 싶은 단어의 인덱스는 1로 나머지는 전부 0으로 표현하는 간단한 벡터이다. 

> 예를 들어 I love you를 원-핫 인코딩 한다면 [1, 0, 0], [0, 1, 0], [0, 0, 1]이다.

이러한 원-핫 인코딩의 한계로는 단어 집합의 크기가 커질 수록 벡터의 크기가 점점 증가하여 저장공간 측면에서 비효율적이고, 단어 사이에서 유사도를 구할 수가 없는 표현방법이라는 부분이 단점이다. 이러한 단점을 보완한 방법은 추후에 다루어 보도록 하자.



지금까지 자연어 처리에서 텍스트 전처리에 대하여 이론적인 부분을 다뤄보았다. 기초적인 부분이라 실무적으로 필요성이 적겠지만 앞으로 배울 복잡한 내용의 기초가 될 수 있어 오래 걸리더라도 높은 이해가 필요할 것 같다. 복잡할수 있지만 이해하고 유기적으로 연결하면서 점점 흐름을 알게되는 것 같다. 실습 코드는 필요하다고 느끼면 다뤄볼 생각이다.