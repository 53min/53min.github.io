---
title: "[인공지능] 머신 러닝"
author: <author_id>
date: 2024-02-23 18:00:00 +09:00
categories: [AI, Machine Learning]
math: true
tag: [자연어처리, 머신 러닝, 지도학습, 비지도학습, 과적합, 가설, 가중치, 편향, 손실 함수, 비용 함수, 목적함수, 옵티마이저, 최적화 알고리즘]
toc: true
---

> 이 포스트는 [**딥 러닝을 이용한 자연어 처리 입문**](https://wikidocs.net/book/2155)을 읽고 공부 겸 정리한 글입니다.

## **머신 러닝**

**머신러닝**이란 기존의 프로그래밍 접근 방식에서 한계를 보완하기 위한 새로운 접근 방식이다. 예를 들어, 사진을 주고 이 사진이 강아지인지 고양이인지 구분한다고 하자. 사진을 보고 구분하는 것은 사람에게는 쉬운 일이다. 하지만 이 문제를 프로그램으로 구현할려고 하면 쉽지 않을 것이다. 사진이란 통일된 양식이 아니라 모두 다르기에 모든 사진에 적용하는 알고리즘을 작성하는 것은 불가능할 것이다.

그렇다면 사람은 어떻게 쉽게 구분할 수 있을까? 우리들은 이미 수많은 고양이와 강아지를 보고 무엇이 강아지인지 무엇이 고양이인지 배웠기 때문이다. 새로운 동물을 보았을 때, 이전에 경험한 수많은 강아지와 고양이를 떠올린 후 각각의 특징을 비교하여 판단하기 때문에 쉽게 구분이 가능하다. 

**머신러닝**은 이러한 사람의 방식을 기계에 적용한 것이다. 기계에게 사진과 이 사진이 어떤 동물인지를 묶어서 충분한 양의 데이터와 해답을 전달하면 기계가 스스로 규칙성을 발견할 것이다. 데이터가 주어지고 기계가 규칙성을 찾는 과정을 **훈련** 또는 **학습**이라고 한다.

기계가 학습을 하면서 발견한 규칙성을 토대로 새로운 사진을 보았을 때 적절한 해답을 제시할 것이다. 이러한 접근법은 기존의 프로그래밍 방식으로는 해결하기 어려웠던 문제들의 해결책이 되기도 한다.

### 데이터의 분리

위의 개념에서 알 수 있듯이 머신러닝에서 데이터는 아주 중요한 역할이다. 머신 러닝을 위한 데이터를 충분히 수집했다면 보통 그 데이터를 분리하여 사용한다. 머신러닝의 목적은 데이터를 이용해 학습시킨 후 새로운 데이터가 주어졌을 때 그 데이터에 대한 적절한 해답을 주는 것이다. 그래서 준비한 데이터를 각각 훈련, 검증, 테스트 데이터로 분리한다.

> 쉽게 말해서 훈련 데이터는 문제집, 검증 데이터는 모의고사, 테스트 데이터는 수능이라고 생각하면 편하다.

훈련데이터에 대해서는 성능이 좋지만 테스트 데이터에 대한 성능이 떨어지는 것을 **과적합(overfitting)**이라고 하는데 검증 데이터는 과적합 판단과 하이퍼 파라미터의 조정을 통한 정확도 향상을 위한 것이고 테스트 데이터는 모델의 최종 성능을 평가하는 것이다.

 **하이퍼 파라미터(초매개변수)**란 모델 성능에 영향을 주는 변수로 사람이 직접 지정하는 값이다. 반면 가중치와 편향같이 모델이 학습하면서 계속 변화하는 변수를 **매개변수**라고 한다. 또한 하이퍼 파라미터를 조정하는 것을 **튜닝(tuning)**한다고 부른다.

또한 한 데이터에서 독립변수들을 **특성(Feature)**이라고 하고 여러개의 데이터에서 한 데이터를 **샘플(Sample)**이라고 한다. 행렬 관점으로 보면 행을 샘플 열을 특성이라고 할 수 있다.

### 머신러닝이 적용되는 문제

머신러닝이 적용되는 문제는 대부분 분류 또는 회귀이다. 회귀 문제는 연속적인 수의 범위 내에서 예측값을 계산하는 것을 말하고 분류 문제는 여러 개의 선택지 중 가장 적절한 선택을 하는 걸 말한다. 분류 문제 중에서도 두가지 선택지가 주어진 경우를 **이진 분류 문제(binary Classification)**, 세 개 이상의 선택지가 주어진 경우를 **다중 클래스 분류(Multi-class Classification)**라고 한다.

> 회귀 문제의 예시로는 주가의 예측, 분류 문제의 예시로는 정상 메일, 스팸 메일 판단이 있다.

### 학습 방법

머신 러닝의 학습 방법은 크게 지도 학습, 비지도 학습, 강화 학습으로 나뉜다.

**지도학습(Supervised Learning)**이란 데이터에 정답이 포함되어 학습하는 것이다. 대부분의 자연어처리는 지도학습에 속하고 데이터의 정답을 레이블(Label)이라고 부른다.

**비지도학습(Unsupervised Learning)**이란 데이터에 레이블이 없이 학습하는 것이다. 여러개의 데이터가 있고 비슷한 데이터끼리 군집을 형성시키려고 할 때 비지도 학습을 이용하기도 한다.

**강화학습(Reinforcement Learning)**이란 시행착오를 통해 실수를 줄이고 보상을 최대화하는 방법으로 학습하는 방식이다.

**자기지도 학습(Self-Supervised Learning, SSL)**이란 큰 갈래의 학습 방법은 아니지만 레이블이 없는 데이터가 주어졌을 때, 모델이 학습을 위해 스스로 레이블을 만들어서 학습하는 방법으로 자연어 처리에서 주로 쓰인다.

## 연결고리

이 내용은 넓은 개념과 정확한 이론 사이에 간극을 채우기 위한 주관적인 내용이다.
위의 내용을 읽었다면 대충 머신러닝에 대한 느낌이 잡혔을 거라 생각한다. 하지만 이 지금부터는 머신러닝 기법들의 개념에 대해 다룰건데, 필자는 '기계가 목적에 맞게 준비한 데이터를 학습하여 규칙성이나 특징을 발견하고 새로운 데이터가 주어졌을 때 적절한 결과를 도출한다'라는 숲같은 개념에서 갑자기 바로 나뭇잎을 배우는 느낌이 들었다. 따라서 중간 단계의 이해를 스스로 정의해보려고 한다. 

내가 가고 싶은 목적지가 있다고 하자. 이 목적지까지 가는 방법으로는 자동차, 자전거, 비행기, 걷기 등 여러가지가 있다. 머신러닝이란 이러한 이동수단 중에 하나라고 생각한다. 그리고 개인적으로 필자는 머신러닝이 이동수단 중에 자동차 정도라고 느꼈으므로, 내가 자동차를 통해 목적지를 가기로 결정했다고 해보자. 보통은 네비게이션으로 목적지를 설정하고 출발할텐데, 목적지를 설정하는 과정에서 많은 옵션들이 있다. 최소 시간 우선, 고속도로 우선, 국도 우선, 무료도로 우선 등이 있는데 이러한 옵션들이 머신러닝 기법에 해당한다고 생각한다. 무료도로 우선을 선택한다면 비용은 확실히 절감되는 반면에 소요 시간이 늘어날 것이고, 최소 시간 우선을 선택하면 소요 시간은 된축되더라도 어느정도 비용이 소모될 것이다. 결국 모든 선택지가 장단점이 있기에 주어진 상황과 목적에 따라 손해를 최소화하고, 이익을 극대화하는 선택지를 고르는 것이다. 이 내용은 머신러닝을 넘어 프로그래밍 전반에 적용된다고 느낀다.

이 다음은 머신러닝에서 자주 쓰이며, 기법들을 다루기 전에 미리 알고 가면 좋은 개념들이다.

## 머신러닝 개념

### 가설(Hypothesis)

수학적 용어지만 머신 러닝에서는 살짝 다르다. 가설이 곧 내가 사용할 기법을 의미한다고 볼 수 있으며, 내가 가진 목적을 전제로 가장 **데이터들 사이의 관계를 잘 표현할 수 있는 함수 또는 식**이다. 지도학습에서는 영향을 주는 독립변수와 영향을 받는 종속 변수의 관계로 표현되어, 독립변수를 x, 종속 변수를 y로 설정하며 y = ~ 형태로 표현된다. 비지도학습의 경우에는 종속변수 없이 독립변수들 사이에 관계가 식으로 표현되기도 한다. 각 기법마다 데이터 사이의 관계가 다르게 표현되어 내 목적에 맞는 가설을 설정해야한다.

### 가중치(Weight)와 편향(Bias)

위에서 가설이 잘 설정되었다고 하였다. 가중치와 편향이란 이 관계를 직접적으로 표현해주는 **매개변수** 이다. 가중치는 w, 편향은 b라고 표현하며, 만약 지도학습에서 x와 y가 직선 관계라면, y = wx + b라고 표현한다. 가중치는 독립변수인 x에 곱해져 직접적으로 영향을 주고, 편향은 가중치와 곱해진 독립변수와 종속변수 사이를 보정해준다고 생각하면 된다. 이전 포스트에서 머신러닝의 학습 방법을 설명하였다. 가중치와 편향은 모델이 학습하면서 오차를 줄이고 정확도를 높이는 방향으로 점차 변한다.

### 손실함수(Loss Function)

위의 개념을 통해 데이터 사이의 관계가 w와 b를 통해 표현된다는 것을 알았고, w와 b가 학습을 통해 점점 최적화된다는 것을 알게되었다. 그렇다면 결국 학습의 목적은 적절한 w와 b를 찾아가는 것이다. 준비한 데이터를 통해 실제값이 있을 것이고, 가설에 데이터를 대입하여 예측값을 얻을 수 있다. 손실함수란 실제값과 예측값의 **오차를 계산하는 식**을 표현한 것이다. 또한 단순히 오차를 계산하는 것에 그치지 않고 오차를 줄이는 작업에 최적화 된 식이여야만 한다. **비용함수(Cost function)**라고 부르기도 하며, 함수의 값을 최소화 또는 최대화하는 목적을 가진 함수를 뜻하는 **목적함수(Objective function)**라고 하기도 한다.

### 최적화 알고리즘(Optimizer, 옵티마이저)

위에서 손실함수란 예측값과 실제값 사이의 오차를 줄이는 작업에 최적화 된 식이라고 정의하였다. 이러한 오차를 최소화하는 작업을 수행하고 결국 오차를 최소화하는 매개변수인 w와 b를 찾는 일을 해야하는데 이러한 작업에 사용되는 알고리즘을 **옵티마이저(Optimizer)** 또는 **최적화 알고리즘**이라고 부른다. 결국 머신러닝에서 훈련 또는 학습은 옵티마이저를 통해 적절한 w와 b를 찾아가는 것을 이야기 한다.

위의 개념들을 통해 머신러닝을 정의해보려고 한다.

머신러닝이란 목적에 맞게 내가 준비한 데이터들 사이의 관계를 가장 잘 표현하는 가설을 설정하고, 가설과 목적에 맞는 손실함수와 옵티마이저를 정한다. 이후 데이터를 통해 손실함수로 오차를 계산하고 옵티마이저를 통해 오차를 줄이는 쪽으로 가중치와 편향을 업데이트하는 것을 반복한다. 이후에 새로운 데이터를 모델에 입력하고 출력된 결과를 평가하는 것이다.

### 과적합(Overfitting)과 과소적합(Underfitting)

머신러닝의 학습과정은 보통 주어진 학습 데이터를 여러번 반복학습하며 점차 성능을 향상시켜 나가는 방식이다. 학습 데이터를 반복하는 횟수를 **에포크(epoch)**라고 하며 학습 데이터를 한번씩 모두 학습할 때마다 1 epoch이다. 또한 에포크는 사용자가 직접 설정하기에 하이퍼 파라미터에 속한다.

학습을 진행하면 학습 데이터에 대한 성능은 높아질 수 밖에 없다. 하지만 머신러닝의 목적은 결국 모델을 학습시키고 새로운 데이터에 대하여 좋은 성능을 내는 것인데 학습 데이터를 과하게 학습하면 학습 데이터에 대한 성능은 높고, 새로운 데이터에 대한 실제 성능은 떨어지게 된다. 이러한 현상을 **과적합**이라고 하는데 과적합이 발생한 모델은 실제 서비스가 좋지 않기 때문에 결코 좋은 모델이라고 할 수 없다.

반면에 성능이 더욱 올라갈 여지가 있음에도 훈련을 덜 한 상태를 **과소적합**이라고 한다. 보통은 에포크가 적게 설정되어 훈련이 부족한 상태일 때 발생하며, 과적합과 다르게 훈련 데이터에 대한 성능도 좋지 않다.

과적합과 과소적합같은 이름이 주어지게 된 이유는 머신 러닝에서 학습 또는 훈련을 하는 과정을 **적합(fitting)**이라고 하기 때문이다. 과적합을 막을 수 있는 방법으로는 드롭 아웃(Dropout), 조기 종료(Early Stopping) 등이 있고 이 내용은 추후에 다룰 예정이다.

이러한 과적합과 과소적합같은 문제를 방지하기 위하여 테스트 데이터를 분리하여 사용하곤한다. 한 테스트 데이터는 과적합 모니터링과 하이퍼 파라미터 튜닝, 다른 테스트 데이터를 모델의 성능 평가를 위해 사용한다. 그리고 전자의 테스트 데이터를 앞서 언급한 검증 데이터라고 부른다.

과적합을 막기 위해 훈련 데이터의 1 epoch 학습 마다 검증 데이터로 정확도와 오차를 계산하고, 만약 오차가 이전보다 증가한다면 학습을 종료하고 테스트 데이터로 모델을 평가하는 방법이 있다. 이 방식을 조기 종료라고 한다.

### 혼동 행렬(Confusion Matrix)

**혼동 행렬**이란 단순히 전체 문제 수에서 맞춘 문제 수의 비율인 정확도의 개념에서 맞춘 결과에 틀린 결과에 대한 정보를 더한 확장된 개념이다. 혼동 행렬의 각 열은 예측값이고, 각 행을 실제 값을 나타낸다. 참과 거짓을 예측하는 문제라고 하면 혼동 행렬은 아래와 같다.

|          | 예측 참 | 예측 거짓 |
|:---------|:--------|:---------|
| 실제 참   | TP     | FN       |
| 실제 거짓 | FP     | TN       |

다음은 머신 러닝에서 정의하는 TP, FP, FN, TN이다. 이 개념을 이용하여 정밀도(Precision),  재현율(Recall)과 정확도(Accuracy)를 계산한다.

- True Positive(TP): 실제 True인 정답을 True라고 예측(정답)
- False Positive(FP): 실제 False인 정답을 True라고 예측(오답)
- False Negative(FN): 실제 True인 정답을 False라고 예측(오답)
- True Negative(TN): 실제 False인 정답을 False라고 예측(정답)

**정밀도**란 모델이 True라고 분류한 것 중 실제 True의 비율이다.

$$  정밀도 = {TP \over TP + FP}  $$

**재현율**이란 실제 True 중 모델이 True라고 예측한 비율이다.

$$  재현율 = {TP \over TP + FN}  $$

**정확도(Accuracy)**는 우리에게 가장 익숙한 지표로 전체 데이터에서 정답을 맞춘 비율이다.

$$  정확도 = {TP + TN \over TP + FN + FP + TN}  $$

정밀도와 재현율은 모두 실제 True를 True라고 예측한 TP에 초점을 둔 개념이다. 정확도는 익히 사용되는데 성능을 측정하기에 적절하지 않을 때가 있다. 예를 들어 우천을 예측하는 모델을 만들었다고 하자. 100일동안 총 3일 비가왔는데, 모델은 100일 내내 맑았다고 예측했다. 이런 경우에는 100번 중 97번을 정답을 맞췄기에 정확도는 97%이지만 정작 하나도 못 맞춘 셈이다. 이렇게 더 중요한 데이터가 적은 비율을 차지하는 경우 정확도는 좋은 측정 지표가 될 수 없다.

개념위주로 글을 작성하여 기법들과 함께 예시를 들으려고 했으나, 내용이 길어져 이 개념을 적용한 머신러닝 기법들은 다음 포스트에서 다루려고 한다.