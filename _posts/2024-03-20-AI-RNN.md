---
title: "[인공지능] 순환 신경망"
author: <author_id>
date: 2024-03-25 18:00:00 +09:00
categories: [AI, RNN]
math: true
tag: [자연어처리, 딥러닝, 신경망, 순환 신경망, RNN, LSTM, GRU, 장단기 메모리, 게이트 순환 유닛]
toc: true
---

> 이 포스트는 [**딥 러닝을 이용한 자연어 처리 입문**](https://wikidocs.net/book/2155)을 읽고 공부 겸 정리한 글입니다.

## 순환 신경망(Recurrent Neural Network, RNN)

피드 포워드 신경망에는 한계점이 있는데, 바로 입력의 길이가 고정된다는 점이다. 이는 자연어 처리를 신경망에 작용하는데에 있어서 치명적인 결점이다. 따라서 원활한 자연어 처리 작업을 위하여 입력의 길이가 유동적인 인공 신경망이 필요하였는데, 순환 신경망(Recurrent Neural Network, RNN)이 대표적인 예시이다.

순환 신경망은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델이다. 피드 포워드 신경망은 은닉층의 값이 오직 출력층으로만 향한다. 하지만 순환 신경망은 은닉층에서 활성화 함수를 통해 나온 결과값이 출력층 방향으로 감과 동시에 다시 은닉층으로 향하여 다음 계산에서 사용된다. 이러한 역할을 하는 은닉층 노드를 셀(cell)이라고 하며, 이전 계산에 대한 정보를 기억하고 있으므로 메모리 셀 또는 RNN 셀이라고 표현한다.

순환 신경망에서는 시퀀스 단위로 처리하는데, 각 시퀀스를 시점이라고 표현하기도 한다. 따라서 은닉층은 현재 시점의 계산된 결과값을 다음 시점으로 보내게 되는데, 이 값을 은닉 상태(hidden state)라고 한다. 현재 시점이 t라면 t 시점의 메모리 셀은 t-1 시점의 메모리 셀에서 계산된 은닉 상태의 값을 입력값으로 사용한다.

따라서 순환 신경망은 각 시점에서 두개의 입력값을 가지고 출력층으로 향한다. 하나는 이전 시점의 정보인 은닉 상태이고 다른 하나는 현재 시점의 입력값이다 따라서 가중치도 두개가 존재한다. 이를 표현한 이미지와 수식은 아래와 같다. 사용하는 활성화 함수는 주로 하이퍼볼릭탄젠트 함수이다.

![Desktop View](/assets/img/rnn계산.png)
_RNN에서 연산과정_

$$ 은닉층: h_t = tanh(W_x x_t + W_h h_{t-1} + b) $$

$$ 출력층: y_t = f(W_y h_t + b) , f는 비선형 함수 $$

또한 입력과 출력의 길이가 유동적이라 목적에 따라 다양한 용도로 설계가 가능하다. 아래 사진은 입력과 출력의 길이에 따른 RNN의 종류이다. 각 시점의 단위는 보통 단어 벡터이다.

![Desktop View](/assets/img/rnn종류.png)
_RNN 구조_

일 대 다 구조의 예시는 이미지 입력에 대해 제목을 출력하는 이미지 캡셔닝이 있다. 제목은 단어 시퀀스이기 때문이다. 다 대 일 구조의 예시로는 스팸 메일 분류가 있고, 다 대 다 구조의 예시로는 번역기 또는 품사를 매칭하는 품사 태깅같은 작업이 있다.

## 장단기 메모리(Long Short-Term Memory, LSTM)

![Desktop View](/assets/img/rnn내부.png)
_RNN 내부_

위의 RNN은 가장 단순한 형태의 RNN인데, 이 RNN을 vanilla RNN이라 칭한다. 바닐라 RNN은 현재 시점의 계산 결과가 이전 시점에 이루어진 계산 결과에 의존하게 된다. 하지만 시퀀스가 길어져 시점이 계속될 수록 앞의 정보가 뒤로 충분히 전달되지 않는 현상이 발생한다. 이는 기울기 소실과 유사한 늬앙스이며, 초기 정보의 영향력이 희미해지는 것을 의미한다. 이는 시퀀스가 길어질수록 두드러지며 장기 의존성 문제(the problem of Long-Term Dependencies)라고 한다.

LSTM은 이러한 한계를 극복하기 위한 RNN의 변형이다. LSTM은 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억은 제거하고, 중요한 기억은 저장한다. 셀 상태(cell state)라는 값을 추가하여 은닉 상태를 계산하며, 긴 시퀀스의 입력에도 좋은 성능을 보인다.

입력 게이트의 계산값을 통하여 현재 시점의 입력의 반영 정도를 결정하고, 삭제 게이트의 계산값을 통하여 이전 셀 상태의 반영 정도를 결정한다. 출력 게이트에서는 현재 시점의 입력값과 이전 시점의 은닉 상태가 활성화 함수를 통하여 계산되고, 출력 게이트의 결과 값은 현재 시점의 은닉 상태를 보완하는데 쓰이고 은닉 상태는 출력층과 다음 시점의 은닉층으로 향한다. 

입력 게이트와 삭제 게이트를 통하여 현재의 셀 상태를 결정하고, 출력 게이트와 셀 상태를 통하여 현재 시점의 은닉 상태를 계산한다. 입력, 삭제, 출력 게이트에서는 모두 이전 시점의 은닉 상태를 통해 계산되고 현재 시점의 셀 상태도 이전 시점의 셀 상태를 통하여 계산되어, RNN의 큰 틀은 유지하며 파라미터가 다양해지고, 연산 과정이 복잡해졌다는 점이 차이점이다. 각 게이트에 자세한 연산 과정은 다음과 같다.

### 입력 게이트

![Desktop View](/assets/img/입력게이트.png)
_입력게이트_

현재 정보를 기억하기 위한 게이트이다. i와 g를 통하여 기억할 정보의 양을 셀 상태를 계산할 때 결정한다.

### 삭제 게이트

![Desktop View](/assets/img/삭제게이트.png)
_삭제게이트_

기억을 삭제하기 위한 게이트이다. 시그모이드 함수를 거쳐 0과 1사이의 결과값을 얻는데, 0에 가까울 수록 많이 삭제된 것이고, 1에 가까울수록 정보를 기억한 것이다.

### 셀 상태

![Desktop View](/assets/img/셀상태.png)
_셀 상태_

원소별 곱을 통하여 입력 게이트에서 선택된 기억과 삭제 게이트의 결과값을 더한다. 만약 f값이 0이 된다면 이전 시점의 셀 상태는 영향력이 0이 되고, i값이 0이라면 현재 시점의 셀 상태는 이전 시점에 셀 상태의 값에 의존한다.

### 출력 게이트와 은닉 상태

![Desktop View](/assets/img/출력게이트.png)
_출력게이트_

시그모이드 함수를 통한 결과값을 통해 현재 시점의 은닉 상태를 결정하고, 셀 상태의 값이 tanh 함수를 통하여 출력 게이트와 연산되면서, 값이 걸러지는 효과를 얻게 된다.

## 게이트 순환 유닛(Gated Recurrent Unit, GRU)

GRU는 장기 의존성 문제를 해결한 LSTM의 해결책은 유지하면서, 은닉 상태를 업데이트하는 연산을 줄였다. 즉, 성능은 LSTM과 유사하면서 구조를 간단화 시켰다고 볼 수 있다. GRU는 업데이트 게이트와 리셋 게이트만 존재하며, 학습 속도는 LSTM보다 빠르며 성능 비슷하다고 알려져 있다.

경험자의 말에 따르면 데이터 양이 적을 때는 매개변수의 수가 적은 GRU가 낫고, 데이터 양이 많으면 LSTM이 낫다고 한다. 성능적인 면에서 큰 차이가 없기에 상황에 따라 LSTM과 GRU 중에 골라서 사용하면 되겠다. 다음은 GRU의 연산 과정이다.

![Desktop View](/assets/img/GRU.png)
_GRU_



기본적인 RNN과 이를 변형한 LSTM, GRU를 다루어보았다. 이 모델을 통해 다양한 자연어 처리 문제를 풀 수 있어 실습을 진행해 보며 직접 구현해보면 좋을 거라 생각한다.